---
title: Direct Clustering
subtitle: Statistical Programming using `R`
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 4
---


```{r}
library(reshape2)
library(ggplot2)
library(FactoMineR)
```

## Direct clustering

### Package `stats`

We use the `kmeans()` function, in the package `stats` (already installed and loaded in each `R` session).

#### Default use 

Example with `iris` data and `3` clusters

```{r}
res = kmeans(iris[-5], 3)
res
names(res)
table(res$cluster)
res$size
res$centers
res$totss # I
res$tot.withinss # W
res$betweenss # B
```


#### Advanced use

With multiple randoms initialization, and on a standardized dataset

```{r}
res.mult = kmeans(scale(iris[-5]), 3, nstart = 30, iter.max = 20)
res.mult
```

With defind starting points (here, the means of each specie)

```{r}
m = aggregate(. ~ Species, iris, mean)
res.init = kmeans(iris[-5], m[-1])
res.init
```

### Package `cluster`

This package is already installed with `R`.

```{r}
library(cluster)
```


#### Fuzzy analysis clustering

`fanny()` computes a fuzzy clustering int $k$ clusters (see `?fanny` for more details).

```{r}
res.fanny = fanny(iris[-5], 3)
names(res.fanny)
table(res.fanny$clustering)
head(res.fanny$membership)
plot(res.fanny)
```


#### Partitioning around medoids

`pam()` performs a partitioning of the data in $k$ clusters, using *medoids*  instead of *means*, to have more robust results (see `?pam` for more details).

```{r}
res.pam = pam(iris[-5], 3)
res.pam
table(res.pam$clustering)
res.pam$medoids
```


#### Clustering large applications

`clara()` is able to deal with large datasets (sse `?clara` for more details). It divides the set into subset of fixed size, and apply `pam()` algorithm on each of them.

## Number of clusters

We also use the package `NbClust` to search an interesting number of clusters. We only have the $k$-means method, for direct clustering.

```{r}
library(NbClust)
nb = NbClust(iris[-5], method = "kmeans")
```

We can explore, as for HAC (and with the code), the results for more details

```{r}
t(nb$Best.nc)
par(mfrow = c(4, 7), mar = c(1, 1, 2, 0) + .1)
for (i in 1:ncol(nb$All.index)) {
    plot(rownames(nb$All.index), nb$All.index[,i], type = "l",
         main = colnames(nb$All.index)[i], axes = FALSE)
    axis(1, at = rownames(nb$All.index), labels = rownames(nb$All.index), 
         lwd = 0, padj = -2)
    best = nb$Best.nc[1,i]
    if (best != 0)
        points(best[1], nb$All.index[as.character(best),i], col = "red")
}
```

We get also the best partition

```{r}
nb$Best.partition
table(nb$Best.partition)
```



## Clusters validation and representation

```{r}
table(iris$Species, res.init$cluster)
pairs(iris[-5], col = rainbow(3)[res.init$cluster], pch = 19)
dres = data.frame(iris[-5], cluster = factor(res.init$cluster))
dres.melt = melt(dres, id.vars = "cluster")
ggplot(dres.melt, aes(cluster, value, fill = cluster)) + 
    geom_boxplot() + 
    facet_wrap(~ variable, scales = "free")
pca = PCA(iris, quali.sup = 5, graph = FALSE)
res.pca = data.frame(pca$ind$coord, cluster = factor(res.init$cluster))
ggplot(res.pca, aes(Dim.1, Dim.2, color = cluster)) + 
    geom_point() +
    stat_ellipse()
```


## Some work

As for hierarchical clustering, from the previous `pendigits` data, use direct clustering to find, for each digit, a number of types of writing, and represent them.
